<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Data Science Notes</title>
        <link rel="stylesheet" type="text/css" href="normalize.css">
        <link rel="stylesheet" type="text/css" href="styles.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
                src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    </head>

    <body>
        <header>
            <h1>Data Science Notes</h1>
            <nav>
                <ul>
                    <li><a href="#Introduction">Introduction</a></li>
                    <li><a href="#Overview">Overview</a></li>
                    <li><a href="#DataPreparation">Data Preparation</a></li>
                    <li><a href="#PreformanceEvaluation">Preformance Evaluation</a></li>
                    <li><a href="#LearningMethods">Learning Methods</a>
                        <ul>
                            <li><a href="#kNN">k-Nearest Neighbors</a></li>
                            <li><a href="#NaiveBayes">Naive Bayes</a></li>
                            <li><a href="#LinearRegression">Linear Regression</a></li>
                            <li><a href="#LogisticRegression">Logistic Regression</a></li>
                            <li><a href="#SVM">Support Vector Machines</a></li>
                            <li><a href="#DecisionTrees">Decision Trees</a></li>
                            <li><a href="#NeuralNetworks">Neural Networks</a></li>
                        </ul>
                    </li>
                    <li><a href="#EnsembleMethods">Ensemble Methods</a></li>
                    <li><a href="#AdditionalExamples">Additional Examples</a></li>
                    <li><a href="#References">References</a></li>
                </ul>
            </nav>
        </header>
        <main>
            <article>
                <section>
                    <h2 id="Introduction">Introduction</h2>
                    <p>
                        Data science is a broad field incorporating aspects of mathematics, statistics, and computation to understand and make use of data.
                        The term "data science" is relatively new and doesn't have a precise definition, but there is significant overlap with "artificial intelligence" and "machine learning".
                        There are two main types of data: numerical data and categorical data.
                        Often different techniques are required to work with each type, although it is usually possible to convert back and forth between the two types.
                    </p>
                    <p>
                        In practice, there are two types of tasks to accomplish: supervised learning and unsupervised learning.
                        Supervised learning seeks to learn a function which maps input data to output data.
                        Unsupervised learning seeks to discover patterns in input data.
                        For now these notes will only cover supervised learning.
                        The techniques used for supervised learning problems depend on whether the output data is numerical or categorical.
                        The task is called regression or classification in the case of numerical or categorical data, respectively.
                    </p>
                    <p>
                        The methods used to accomplish a learning task can be classified according to: their ability to update when given additional data and their level of abstractness.
                        In batch learning, the learning process must start from sratch to incorporate new data.
                        In online learning, the learning process will build on what has been learned previously.
                        Instance-based learning involves no abstraction and consists of rote memorization of data.
                        Model-based learning hypothesizes an abstract model which is fit to the data.
                    </p>
                    <p>
                        These differences can strongly effect the practical use of a learning process.
                        Common practical considerations include: training time, prediction time, required memory.
                        <table>
                            <tr>
                                <td></td>
                                <th>Batch Learning</th>
                                <th>Online Learning</th>
                            </tr>
                            <tr>
                                <th>Instance-based</th>
                                <td colspan="2">
                                    <ul>
                                        <li>0 training time</li>
                                        <li>prediction time scales with data size</li>
                                        <li>required memory scales with data size</li>
                                    </ul>
                                </td>
                            </tr>
                            <tr>
                                <th>Model-based</th>
                                <td>
                                    <ul>
                                        <li>can have long training time</li>
                                        <li>prediction time depends on model complexity</li>
                                        <li>required memory scales with data size</li>
                                    </ul>
                                </td>
                                <td>
                                    <ul>
                                        <li>training time can be small with frequent updates</li>
                                        <li>prediction time depends on model complexity</li>
                                        <li>required memory scales with new data size</li>
                                    </ul>
                                </td>
                            </tr>
                        </table>
                    </p>
                </section>
                <section>
                    <article>
                        <header>
                            <h2 id="Overview">Overview</h2>
                            <p>
                                We will work through a simple data science example to give an overview of the process and ideas.
                                A well-known source of data sets is the <a href="https://archive.ics.uci.edu/ml/index.php">UCI Machine Learning Repository</a>.
                                Here we will find the Iris data set, one of the most popular for beginners.
                                Computations and visualizations will be carried out in <a href="https://jupyter.org/">Jupyter Notebook</a> with <a href="https://www.python.org/">Python</a> using standard packages including <a href="https://numpy.org/">NumPy</a>, <a href="https://pandas.pydata.org/">Pandas</a>, <a href="https://matplotlib.org/">Matplotlib</a>, <a href="https://seaborn.pydata.org/">Seaborn</a>, and <a href="https://scikit-learn.org/stable/">SciKit learn</a>.
                            </p>
                            <nav>
                                <ol>
                                    <li><a href="#GetData">Get the Data</a></li>
                                    <li><a href="#SplitData">Split Data into Training and Testing Sets</a></li>
                                    <li><a href="#ExploreData">Explore the Data</a></li>
                                    <li><a href="#HypothesizeModels">Hypothesize Models</a></li>
                                    <li><a href="#Preprocessing">Clean/Preprocess the Data</a></li>
                                    <li><a href="#TrainModels">Train Models</a></li>
                                    <li><a href="#Evalutaion">Evaluation</a></li>
                                    <li><a href="#OptimizeHyperparameters">Optimize Hyperparameters</a></li>
                                    <li><a href="#ModelSelection">Select Model or Ensemble</a></li>
                                    <li><a href="#FinalEvaluation">Final Evaluation</a></li>
                                    <li><a href="#UseModel">Put in Use</a></li>
                                    <li><a href="#UpdateModel">Update with New Data</a></li>
                                </ol>
                            </nav>
                        </header>
                        <section>
                            <h3 id="#GetData">Get the Data</h3>
                            <p>
                                Let us fix some notation and structure for the data which is collected or downloaded.
                                Our data will be in table/matrix form.
                                Each row will correspond to an example taken from the population, and each column will correspond to a property of the example.
                                Often the final column will contain the output data and be separated from the rest of the input data.
                                We will denote the "matrix" of input data by \( X \) and the "vector" of output data by \( \mathbf{y} \).
                            </p>
                            <p>
                                The downloaded Iris data comes in the form of a csv file.
                                Each line of the file is a row of the matrix and commas separate the entries by column along each row.
                                The first two columns contain the sepal length and width respectively (in cm).
                                The next two columns contain the petal length and width respectively (in cm).
                                The final column contains the class of Iris: Setosa, Versicolour, or Virginica.
                                Note that the first four input properties are numerical while the output is categorical.
                            </p>
                            <p>
                                In jupyter, load the data with using
                                <pre>
                                    <code>
                                        data = pd.read_csv('iris.data', header=None, names=['SL', 'SW', 'PL', 'PW', 'C'])
                                    </code>
                                </pre>
                                Separate the input data from the output data using
                                <pre>
                                    <code>
                                        X = data.drop(['C'], axis=1)
                                        y = data[['C']].copy()
                                    </code>
                                </pre>
                                <aside>
                                    A common warning when working with Pandas DataFrames is "Setting with Copy Warning".
                                    The <code>.drop()</code> method will return a copy, and no warning is given when modifying.
                                    The expression <code>data[['C']]</code> also seems to return a copy, but to prevent the warning explicitly use the <code>.copy()</code> method.
                                </aside>
                            </p>
                        </section>
                        <section>
                            <h3 id="#SplitData">Split Data into Training and Testing Sets</h3>
                            <p>
                                In order to evaluate how well the machine learned the task, we must set aside some data to be used for testing.
                                It is important to not look at or use the test data so that the model is not overfit to these specific tests.
                                Such overfitting would leave us with misplaced confidence in our model when applying it to new data.
                            </p>
                            <p>
                                There is some freedom in how much of the data should be set aside.
                                A good rule of thumb is to use 20% for testing and 80% for training.
                                The simplest way to split is with uniform random selection.
                                However, sometimes it is desirable for the distribution of the test data to match the distribution of all the data.
                                A stratified split can be used to preserve a distribution.
                            </p>
                            <p>
                                In jupyter, we may split the data by selecting 20% uniformly at random using
                                <pre>
                                    <code>
                                        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                                    </code>
                                </pre>
                                Alternatively, if we want the input and output data in the same DataFrame
                                <pre>
                                    <code>
                                        data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)
                                    </code>
                                </pre>
                                <aside>
                                    As we will see some library functions expect all the data to be in the same DataFrame.
                                </aside>
                            </p>
                        </section>
                        <section>
                            <h3 id="#ExploreData">Explore the Data</h3>
                            <p>
                                Now that we have data from which the machine is ready to learn, we should explore the data.
                                The purpose of the exploration is to hypothesize which models and techniques will be the best to use.
                                Additionally, our goal in this phase is to identify problems with the data such as missing values, typographical errors, inconsistant formatting, outliers, and skewed distributions.
                            </p>
                            <p>
                                There are a handful of simple commands which will provide basic insights into the data.
                                The <code>.shape</code> property of a pandas DataFrame will display the number of rows and columns.
                                The <code>.head()</code> method of a DataFrame will display the first 5 rows of data.
                                The <code>.info()</code> method of a DataFrame is very useful, it will display the data type of each column and give the number of non-missing values in each column.
                                The <code>.describe()</code> method of a DataFrame will give a statistical summary (mean, standard deviation, min, max, and quartiles) of each numerical data column.
                                A seaborn function <code>sns.pairplot()</code> will create scatter plots for each pair of numerical columns as well as histograms for each numerical column.
                                <pre>
                                    <code>
                                        print('DataFrame Shape: ', data_train.shape)
                                        print('\nFirst 5 rows of DataFrame:\n', data_train.head())
                                        print('\nColumn Data Types and Number of Non-Missing Values:')
                                        data_train.info()
                                        print('\nStatistical Summary of Numeric Columns:\n', data_train.describe())
                                        sns.pairplot(data_train);
                                    </code>
                                </pre>
                                The output from <code>.shape</code> tells us there are 120 rows and 4 columns.
                                The output from the <code>.info()</code> method indicates that all 4 columns are numerical and that none of the entries are missing values.
                                The pairplot produces looks like
                                <figure>
                                    <img src="images/basic_pairplot_output.png" alt="Pairplot of Numerical Columns">
                                    <figcaption>
                                        Rows and columns of the pairplot are the numerical columns in the DataFrame. Along the diagonal are the histograms for the numerical columns and off the diagonal are scatter plots for each pair of numerical columns.
                                    </figcaption>
                                </figure>
                            </p>
                            <p>
                                When there is no numerical data within a DataFrame <code>.describe()</code> behaves slightly differently.
                                Furthermore, scatter plots should be replaced with frequency tables, and histograms replaced with countplots.
                                The code below gives simple insights to the categorical target column of iris-species
                                <pre>
                                    <code>
                                        print('DataFrame Shape: ', y_train.shape)
                                        print('\nFirst 5 rows of DataFrame:\n',y_train.head())
                                        print('\nColumn Data Types and Number of Non-Missing Values:')
                                        y_train.info()
                                        print('\nStatistical Summary of a Categorical Column:\n', y_train.describe())
                                        y_test.groupby(['C']).size()
                                        sns.countplot(y_train['C']);
                                    </code>
                                </pre>
                                The <code>.countplot()</code> output looks like
                                <figure>
                                    <img src="images/countplot_output.png" alt="Countplot of Categorical Variable">
                                    <figcaption>
                                        Plot showing the number of iris of each species.
                                    </figcaption>
                                </figure>
                            </p>
                            <p>
                                In some of the scatter plots we can see clusters.
                                In order to gain a deeper understanding of these patterns we should incorporate the species of each data point.
                                Either of the following commands will add color to the pairplot, so that the data points for the different species (Setosa, Versicolor, and Virginica) are colored differently.
                                <pre>
                                    <code>
                                        sns.pairplot(data_train, hue='C');
                                        sns.pairplot(pd.concat([X_train, y_train], axis=1), hue='C');
                                    </code>
                                </pre>
                                The output will look like
                                <figure>
                                    <img src="images/colored_pairplot_output.png" alt="Pairplot of Numerical Columns With Points Colored by Species">
                                    <figcaption>
                                        Rows and columns of the pairplot are the numerical columns in the DataFrame. Along the diagonal are the histograms for the numerical columns and off the diagonal are scatter plots for each pair of numerical columns. Each point is colored according to the species of that example.
                                    </figcaption>
                                </figure>
                                These scatter plots make it clear that we should be able to identify an iris as iris-setosa by measuring its petals and sepals.
                            </p>
                        </section>
                        <section>
                            <h3 id="#HypothesizeModels">Hypothesize Models</h3>
                            <p>
                                Now that we know a bit about the data, we should be able to start hypothesizing which type of learning techniques could work well or could be applied easily.
                                For the iris data, we are faced with a classification task, namely, given the length and width of the petals and sepals of an iris determine which species it belongs to: setosa, versicolor, or virginica.
                                For classification we could use almost any method including k-nearest neighbors, naive bayes, logistic regression, support vector machines, decision trees, or neural networks.
                                Some of these methods in their original form apply only to binary classification tasks, but we are dealing with a multiclass classification problem since there are 3 species.
                            </p>
                            <p>
                                Our input data which is entirely numerical is already in the form required for all these methods except for some naive bayes methods which require Boolean input data.
                                To avoid the preprocessing needed to convert our numeric data in Boolean data we might not use a naive bayes method, but the scikit-learn implementation will attempt to automatically convert for us.
                            </p>
                            <p>
                                Since there is such a small amount of data, computation time and required memory are not limiting considerations.
                                Furthermore, as the data requires little to no data type conversions or preprocessing, we could try all methods and see which preforms the best or else combine several into an ensemble.
                                For the purposes of this overview, we will only use the k-nearest neighbors algorithm.
                            </p>
                        </section>
                        <section>
                            <h3 id="Preprocessing">Clean/Preprocess the Data</h3>
                            <p>
                                One of the most difficult preprocessing steps is not needed since there are no missing values.
                                Most of the learning techniques will not work when missing data is present.
                                There are two simple approaches for missing data.
                                One is to simply remove rows and/or columns with missing data, the drawback here is that data is valuable resource which should not be wasted.
                                The other simple approach fills missing data entries with the mean, median, or mode.
                                More complicated approaches could involve additional learning tasks which try to predict the missing values from the known values.
                            </p>
                            <p>
                                Another common preprocessing step is to convert data types, especially encoding categorical data numerically.
                                Since our input data is already numerical we will not need to convert categorical data to numerical data.
                            </p>
                            <p>
                                The only preprocessing which might be appropriate in this example is to apply transformations to the numerical data.
                                One common transformation is the z-transform which mean centers and normalizes by the standard deviation, the result is comparable to the standard normal distribution.
                                \[ Z = \frac{X - \mu}{\sigma} \]
                                Another common transformation is normalize which subtracts the min and divides by the range, the result is contained within the interval \( [0, 1] \).
                                \[ N = \frac{X - m}{M - m} \]
                                Other common transformations include \( \ln(X) \) or \( \frac{1}{X} \).
                            </p>
                            <p>
                                The k-nearest neighbors algorithm is sensitive to inconsistant scales along different axes so it could be helpful to normalize or z-transform the data before using this method.
                                Likewise when training other methods using gradient descent, training time will be greatly reduced by normalizing or z-transforming the data.
                                Hence, these are the two transformations we may use when preprocessing the data.
                            </p>
                            <p>
                                The scikit-learn package includes several preprocessing capabilities.
                                For example the following code will z-transform the training data,
                                <pre>
                                    <code>
                                        from sklearn.preprocessing import StandardScaler
                                        stdscaler = StandardScaler()
                                        stdscaler.fit(X_train)
                                        X_train_scaled = pd.DataFrame(stdscaler.transform(X_train), columns = X_train.columns, index=X_train.index)
                                    </code>
                                </pre>
                                One of the nice things about implementing the z-transform as a class is that the test data can be transformed using the mean and standard deviation of the training data.
                                <pre>
                                    <code>
                                        X_test_scaled = pd.DataFrame(stdscaler.transform(X_test), columns = X_test.columns, index=X_test.index)
                                    </code>
                                </pre>
                            </p>
                        </section>
                        <section>
                            <h3 id="TrainModels">Train Models</h3>
                            <p>
                                Just to get a feel for things we will train the k-nearest neighbors model with default settings for the hyperparameters.
                                Using scikit-learn it is very simple to train models, import the class, create an instance, and then call the <code>fit</code> method.
                                <pre>
                                    <code>
                                        from sklearn.neighbors import KNeighborsClassifier
                                        knn = KNeighborsClassifier()
                                        knn.fit(X_train, y_train['C'])
                                    </code>
                                </pre>
                            </p>
                        </section>
                        <section>
                            <h3 id="Evaluation">Evaluation</h3>
                            <p>
                                Evaluating the preformance of a model involves several complexities.
                                First of all there are a large number of metrics from which to choose.
                                Secondly it is best to evaluate on new data, but new data is often hard to come by.
                            </p>
                            <p>
                                Frequently cross validation is used to solve the second problem.
                                While we have already set aside test data to be used for evaluation, it should only be used for the final evaluation.
                                So we want to set aside some of the training data for evaluation.
                                Doing a second split would work fine, but with cross validation we instead partition the training data.
                                Then for each one of the partitions the model is trained on the remaining training data and evaluated on the partition.
                                This way all the training data is used to train at some point, and several tests are run.
                                The metrics from these tests can be averaged or otherwise analized statistically to give a more comprehensive view to how the model fits the data.
                            </p>
                            <p>
                                While there are many metrics for evaluating the preformance of multiclass classification problems, here we will just use accuracy.
                                Accuray is simply the number of correctly made predictions out of the total number of predictions made.
                            </p>
                            <p>
                                The following command will partition the training data into five pieces uniformly at random, train the model on all but one piece, then make predictions for all examples, and finally compute the accuracy metric for both data used to train and the data held out for testing.
                                <pre>
                                    <code>
                                        cross_validate(knn, X_train, y_train['C'], cv=5, scoring=['accuracy'], return_train_score=True)
                                    </code>
                                </pre>
                                The output will contain the following
                                <pre>
                                    <code>
                                        'test_accuracy': array([0.96      , 1.        , 0.83333333, 1.        , 0.95652174]),
                                        'train_accuracy': array([0.95789474, 0.96875   , 0.98958333, 0.94791667, 0.96907216])
                                    </code>
                                </pre>
                                The first entry of the arrays tells us about what happened when the first partition was held out for testing and the other four partitions were used for training the model.
                                When classifications were made on the same data from which the model was trained, it was able to correctly classify about 95.8% of the time.
                                The model was able to correctly classify 96% of the examples from the first partition of testing data.
                            </p>
                            <p>
                                Averaging the preformance over the five tests, the k-nearest neighbors method with default settings correctly classifies about 97% of the time on the same data from which the model is trained and about 95% of the time on new data it has not seen before.
                                Hence this method seems to work pretty well.
                            </p>
                        </section>
                        <section>
                            <h3 id="OptimizeHyperparameters">Optimize Hyperparameters</h3>
                            <p>
                                Most models and learning techniques can be varied in some way.
                                These variations are called hyperparameters.
                                Once a promising model is found, it is best to refine the model by choosing the best values for these hyperparameters.
                            </p>
                            <p>
                                For example, for the k-nearest neighbors method, \( k \) the number of nearest neighbors which vote on the class is a hyperparameter.
                                At the extremes when \( k = 1 \), an example is classified to be the same type of the training example nearest to it.
                                And when \( k \) is equal to the number of training examples, every example is classified as the most commonly occuring class.
                                The best value for \( k \) depends on the shape of the data and is something we should try to determine before putting our model into production.
                            </p>
                            <p>
                                Other examples of hyperparameters in the k-nearest neighbors method include the distance metric used to determine distance between examples and the voting method used.
                            </p>
                            <p>
                                To determine optimal values for model hyperparameters, we simply repeate the training and evaluation steps for various values for the hyperparameters, and then select the values with the best preformance metrics.
                                In scikit-learn this can be done with the <code>GridSearchCV</code> class.
                            </p>
                            <p>
                                A first pass could be done with the following commands
                                <pre>
                                    <code>
                                        hyperparameter_grid = {
                                            'n_neighbors': [1, 3, 5, 9, 13, 19, 25, 33, 41],
                                            'weights': ['uniform', 'distance'],
                                            'p': [1, 1.1, 1.5, 1.9, 2, 3, 10, 100, 1000]
                                        }
                                        search = GridSearchCV(knn, param_grid=hyperparameter_grid, scoring='accuracy', cv=5, iid=False)
                                        search.fit(X_train, y_train['C'])
                                        results = pd.DataFrame(search.cv_results_)
                                        results.sort_values(by='mean_test_score', ascending=False)
                                    </code>
                                </pre>
                                The best model for these choices of hyperparameters calculates distance with the formula
                                \[ d(x, y) = \left( \sum_{i} |x_i - y_i|^{100} \right)^{1/100} \]
                                Using this distance formula it finds the 9 nearest neighbors to the example in question, which then each equally vote their own class.
                                The example in question is then classified as the whichever class recieves a plurality vote (ties may be determined by the ordering of the data).
                                This model was correct about 97.5% of the time on new data.
                            </p>
                        </section>
                    </article>
                </section>
            </article>
        </main>
        <footer>
                <aside>
                    <p>
                        Disclaimer: These notes may contain errors, omissions, and inconsistancies.
                        Communicating simply the general ideas is preferred over completeness and technical correctness.
                        This document may contain out-of-date information and should be regarded as a work in progress.
                    </p>
                </aside>
            <p>&copy; 2019 Gerrit Smith</p>
        </footer>
    </body>
</html>